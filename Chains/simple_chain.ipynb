{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Langchain_models\\venv_genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_ACCESS_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Paris is known as the \"City of Light\" (La Ville Lumière) because it was a center of education and ideas during the Age of Enlightenment and also because it was one of the first cities to adopt street lighting.\n",
      "\n",
      "2. The Eiffel Tower, one of the most recognized structures in the world, was built as a temporary exhibit for the 1889 World's Fair to celebrate the 100th anniversary of the French Revolution. It was initially criticized by some of France's leading artists and intellectuals for its design, but it has since become a global cultural icon of France and one of the most recognizable structures in the world.\n",
      "\n",
      "3. Paris is home to the Louvre Museum, the world's largest art museum and a historic monument in Paris. It is home to thousands of works of art, including the Mona Lisa and the Venus de Milo. The museum is housed in the Louvre Palace, which was a royal palace before Louis XIV moved his court to Versailles in 1682.\n",
      "\n",
      "4. Paris has been the setting for many classic films, including French New Wave films of the 1950s and '60s, such as \"The 400 Blows\" and \"Breathless,\" and Hollywood movies like \"Midnight in Paris\" and \"The Da Vinci Code.\"\n",
      "\n",
      "5. The city has a rich culinary heritage and is known for its gourmet food and wines. Paris is home to many famous chefs and Michelin-starred restaurants, and it is also known for its patisseries, boulangeries, and street-side cafes. The city is also famous for its café culture, with many famous writers, artists, and philosophers, including Ernest Hemingway, Pablo Picasso, and Jean-Paul Sartre, having spent time writing and conversing in the city's cafes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template='Generate 5 interesting facts about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "parser=StrOutputParser()\n",
    "chain=prompt|model|parser\n",
    "result=chain.invoke({'topic':'Paris'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grandalf\n",
      "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyparsing in d:\\langchain_models\\venv_genai\\lib\\site-packages (from grandalf) (3.2.3)\n",
      "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "Installing collected packages: grandalf\n",
      "Successfully installed grandalf-0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | ChatHuggingFace |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. The 26/11 attacks in Mumbai, India, were a series of coordinated attacks by the Pakistan-based militant organization Lashkar-e-Taiba (LeT), resulting in 166 deaths and hundreds of injuries.\n",
      "2. The attacks took place at seven locations, including Chhatrapati Shivaji Maharaj Terminus railway station, Leopold Cafe, Cama and Albless Hospital, Nariman House, Taj Mahal Palace Hotel, Oberoi Trident Hotel, and Metro Adlabs cinema.\n",
      "3. The international community condemned the attacks, and Pakistan was accused of being behind them. The Indian government launched an investigation, leading to the identification and sentencing to death of one militant, while nine others were killed.\n",
      "4. The attacks exposed weaknesses in India's security infrastructure and led to calls for greater investment in national security and intelligence capabilities.\n",
      "5. Several memorials have been established in Mumbai to honor the victims, and the Taj Mahal Palace Hotel has been restored, symbolizing Mumbai's resilience and spirit. The attacks underscored the importance of international cooperation in combating terrorism, with a lasting impact on India's national security policies.\n"
     ]
    }
   ],
   "source": [
    "prompt1=PromptTemplate(\n",
    "    template='generate a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template='generate a 5 pointer summary from the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "model=ChatHuggingFace(llm=llm)\n",
    "parser=StrOutputParser()\n",
    "\n",
    "chain=prompt1|model|parser|prompt2|model|parser\n",
    "result=chain.invoke({'topic':'26/11 attacks in Mumbai, India'})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "groq_token = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Set base URL for Groq (OpenAI-compatible)\n",
    "groq_base_url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# Model 1: Mixtral-8x7B (via Groq)\n",
    "model1 = ChatOpenAI(\n",
    "    openai_api_key=groq_token,\n",
    "    openai_api_base=groq_base_url,\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    ")\n",
    "\n",
    "# Model 2: Gemma-7B-IT (via Groq)\n",
    "model2 = ChatOpenAI(\n",
    "    openai_api_key=groq_token,\n",
    "    openai_api_base=groq_base_url,\n",
    "    model_name=\"gemma2-9b-it\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Attention is All You Need: A Brief Overview and Quiz\\n\\n### Introduction\\n\\nThe \"Attention is All You Need\" research paper introduced a new neural network architecture called Transformer. This architecture replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention mechanisms, making it particularly suited for sequence-to-sequence tasks such as machine translation.\\n\\n### Key Components\\n\\n- **Self-Attention Mechanism**: This mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. It takes in a set of query (Q), key (K), and value (V) vectors, computes attention weights by taking the dot product of Q and K and applying a softmax function, and uses these weights to compute a weighted sum of V.\\n- **Encoder-Decoder Architecture**: The Transformer consists of an encoder and a decoder, both composed of identical layers. The encoder takes in a sequence of tokens and outputs a sequence of vectors, while the decoder takes in the output of the encoder and generates a sequence of tokens.\\n- **Multi-Head Attention**: This applies self-attention multiple times in parallel, with different weight matrices. It concatenates the outputs of each attention head and linearly transforms the concatenated output.\\n\\n### Transformer Architecture\\n\\n- **Encoder**: The encoder consists of 6 identical layers, each comprising multi-head self-attention and a feed-forward neural network (FFNN).\\n- **Decoder**: The decoder also consists of 6 identical layers, each comprising multi-head self-attention, encoder-decoder attention, and an FFNN.\\n\\n### Advantages\\n\\n- **Parallelization**: The Transformer can be parallelized more easily than RNNs, making it faster to train.\\n- **Scalability**: The Transformer can handle longer input sequences than RNNs.\\n\\n### Results\\n\\nThe Transformer achieved state-of-the-art results on several machine translation benchmarks, including WMT2014 English-to-German and WMT2014 English-to-French.\\n\\n## Quiz\\n\\n### 1. What is the primary architecture introduced in the \"Attention is All You Need\" paper?\\n\\n**A)** Convolutional Neural Networks (CNNs)  \\n**B)** Recurrent Neural Networks (RNNs)  \\n**C)** Transformer  \\n**D)** Autoencoders  \\n\\n**Answer: C) Transformer**\\n\\n### 2. What mechanism does the Transformer use instead of RNNs and CNNs?\\n\\n**A)** Self-Attention Mechanism  \\n**B)** Convolutional Mechanism  \\n**C)** Recurrent Mechanism  \\n**D)** Autoencoder Mechanism  \\n\\n**Answer: A) Self-Attention Mechanism**\\n\\n### 3. What is the purpose of the Multi-Head Attention in the Transformer?\\n\\n**A)** To apply self-attention sequentially  \\n**B)** To apply self-attention multiple times in parallel with different weight matrices  \\n**C)** To reduce the dimensionality of the input  \\n**D)** To increase the complexity of the model  \\n\\n**Answer: B) To apply self-attention multiple times in parallel with different weight matrices**\\n\\n### 4. What are the advantages of the Transformer over RNNs in terms of training and input sequence length?\\n\\n**A)** Slower training and shorter input sequences  \\n**B)** Faster training and longer input sequences  \\n**C)** Equivalent training speed and input sequence length  \\n**D)** Slower training but can handle longer sequences  \\n\\n**Answer: B) Faster training and longer input sequences**\\n\\n### 5. What were the results of the Transformer on machine translation benchmarks?\\n\\n**A)** Below state-of-the-art results  \\n**B)** At par with state-of-the-art results  \\n**C)** Achieved state-of-the-art results  \\n**D)** Not tested on machine translation benchmarks  \\n\\n**Answer: C) Achieved state-of-the-art results**'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt1=PromptTemplate(\n",
    "    template='generate short and simple notes from the {paper} research paper',\n",
    "    input_variables=['paper']\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template='generate 5 short question answers from the following {paper}',\n",
    "    input_variables=['paper']\n",
    ")\n",
    "prompt3=PromptTemplate(\n",
    "    template='Merge the provided notes and quiz into a single document /n notes-> {notes} and quiz->{quiz}',\n",
    "    input_variables=['notes','quiz']\n",
    ")\n",
    "parser=StrOutputParser()\n",
    "parallel_chain=RunnableParallel({\n",
    "    'notes':prompt1|model1|parser,\n",
    "    'quiz':prompt2|model2|parser\n",
    "})\n",
    "merge_chain=prompt3|model1|parser\n",
    "chain=parallel_chain|merge_chain\n",
    "chain.invoke({'paper':'Attention is All you need'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            +---------------------------+            \n",
      "            | Parallel<notes,quiz>Input |            \n",
      "            +---------------------------+            \n",
      "                 **               **                 \n",
      "              ***                   ***              \n",
      "            **                         **            \n",
      "+----------------+                +----------------+ \n",
      "| PromptTemplate |                | PromptTemplate | \n",
      "+----------------+                +----------------+ \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "  +------------+                    +------------+   \n",
      "  | ChatOpenAI |                    | ChatOpenAI |   \n",
      "  +------------+                    +------------+   \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "+-----------------+              +-----------------+ \n",
      "| StrOutputParser |              | StrOutputParser | \n",
      "+-----------------+              +-----------------+ \n",
      "                 **               **                 \n",
      "                   ***         ***                   \n",
      "                      **     **                      \n",
      "           +----------------------------+            \n",
      "           | Parallel<notes,quiz>Output |            \n",
      "           +----------------------------+            \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                 +----------------+                  \n",
      "                 | PromptTemplate |                  \n",
      "                 +----------------+                  \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                   +------------+                    \n",
      "                   | ChatOpenAI |                    \n",
      "                   +------------+                    \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                +-----------------+                  \n",
      "                | StrOutputParser |                  \n",
      "                +-----------------+                  \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "              +-----------------------+              \n",
      "              | StrOutputParserOutput |              \n",
      "              +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conditional chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some appropriate responses to positive feedback, depending on the context:\n",
      "\n",
      "**Formal:**\n",
      "\n",
      "* \"Thank you for your kind words. I appreciate your feedback.\"\n",
      "* \"We are delighted to hear that you are satisfied with our [product/service]. Your feedback is valuable to us.\"\n",
      "* \"Thank you for taking the time to share your positive experience. We will continue to strive for excellence.\"\n",
      "\n",
      "**Informal:**\n",
      "\n",
      "* \"Thanks so much! I'm really glad you liked it.\"\n",
      "* \"That's awesome to hear, thanks!\"\n",
      "* \"I'm so happy you enjoyed it! Let me know if there's anything else I can do.\"\n",
      "\n",
      "**Adding a Personal Touch:**\n",
      "\n",
      "You can make your response even more personal by:\n",
      "\n",
      "* **Specifying what you're grateful for:** \"Thank you for your kind words about my presentation. I'm especially glad you found the [mention specific part] helpful.\"\n",
      "* **Acknowledging the effort:** \"I really appreciate your feedback. It means a lot to me knowing that my work is valued.\"\n",
      "* **Offering to do more:** \"Thank you for your positive feedback! I'm always looking for ways to improve, so I appreciate your insights.\"\n",
      "\n",
      "**Remember to:**\n",
      "\n",
      "* **Be sincere:** Your response should reflect your genuine appreciation for the feedback.\n",
      "* **Be concise:** Keep your response brief and to the point.\n",
      "* **Be professional:** Even if the feedback is informal, maintain a professional tone.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import Field, BaseModel\n",
    "from typing import Literal\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
    "\n",
    "parser=StrOutputParser()\n",
    "class Feedback(BaseModel):\n",
    "    sentiment: Literal['positive','negative','neutral']=Field(description='Give the sentiment of the feedback')\n",
    "\n",
    "parser2=PydanticOutputParser(pydantic_object=Feedback)\n",
    "\n",
    "prompt1=PromptTemplate(\n",
    "    template=\"classify the sentiment of the following feeback text into positive or negetive or neutral \\n {feedback} \\n {format_instruction}\",\n",
    "    input_variables=['feedback'],\n",
    "    partial_variables={'format_instruction':parser2.get_format_instructions()}\n",
    ")\n",
    "classifier_chain=prompt1|model2|parser2\n",
    "\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template=\"write an appropriate response to this positive feedback \\n {feedback}\",\n",
    "    input_varibles=['feedback']\n",
    ")\n",
    "\n",
    "prompt3=PromptTemplate(\n",
    "    template='write an appropriate response to this negative feedback \\n{feedback}',\n",
    "    input_variables=['feedback']\n",
    "\n",
    ") \n",
    "prompt4=PromptTemplate(\n",
    "    template=\"write an appropriate response to this neutral feedback \\n {feedback}\",\n",
    "    input_varibles=['feedback']\n",
    ")\n",
    "\n",
    "\n",
    "branch_chain=RunnableBranch(\n",
    "    (lambda x:x.sentiment=='positive',prompt2|model2|parser),\n",
    "    (lambda x:x.sentiment=='negative',prompt3|model2|parser),\n",
    "    (lambda x:x.sentiment=='neutral',prompt4|model2|parser),\n",
    "    RunnableLambda(lambda x:\"could not find sentiment\")\n",
    ")\n",
    "\n",
    "chain=classifier_chain|branch_chain\n",
    "print(chain.invoke({'feedback':\"this is a fabulous phone from samsung. The camera is amazing, and the noise cancellation experience on calls is a very good feature to have. ALso I liked its pricing\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
